---
title: "STAT 154 Final Project"
author: "Jiyeon Kim"
output: 
  pdf_document:
          number_sections: false
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
job <- read_csv("job_training_data.csv")
head(job)
dim(job)
```


```{r}
fraud <- job %>% filter(fraudulent == 1)
not_fraud <- job %>% filter(fraudulent == 0)
head(fraud)
head(not_fraud)
dim(fraud)
dim(not_fraud)
colSums(is.na(job))
```

```{r}
# Create the dummy variables for categorical data(employment_type, required_experience, required_education)
library(fastDummies)
categorical <- subset(job, select = c(employment_type, required_experience, required_education))
ctgr_dummy <- dummy_cols(categorical, select_columns=c('employment_type', 'required_experience', 'required_education'),  remove_most_frequent_dummy = TRUE, remove_selected_columns = TRUE)
ctgr_dummy[is.na(ctgr_dummy)] <- 0
colnames(ctgr_dummy) <- gsub(" ", ".", colnames(ctgr_dummy))
colnames(ctgr_dummy) <- gsub("-", ".", colnames(ctgr_dummy))
colnames(ctgr_dummy) <- gsub("'", ".", colnames(ctgr_dummy))
```

```{r}
# Parse out the word features from the complex text features(title, company_profile, description, requirements, benefits)

library(tm)
library(SnowballC)
library(tidytext)

title_corpus <- VCorpus(VectorSource(job$title))
profile_corpus <- VCorpus(VectorSource(job$company_profile))
description_corpus <- VCorpus(VectorSource(job$description))
requirements_corpus <- VCorpus(VectorSource(job$requirements))
benefits_corpus <- VCorpus(VectorSource(job$benefits))

clean_text <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(tolower), lazy = T)
  corpus <- tm_map(corpus, removeNumbers, lazy = T)
  corpus <- tm_map(corpus, removePunctuation, lazy = T)
  corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"), lazy = T )
  corpus <- tm_map(corpus, stripWhitespace, lazy = T )
  corpus <- tm_map(corpus, stemDocument, lazy = T)
  corpus <- tm_map(corpus, stripWhitespace, lazy = T)
  word_freq <- DocumentTermMatrix(corpus)
  remove_sparse <- removeSparseTerms(word_freq, 0.8)
  remove_sparse_df <- as.data.frame(as.matrix(remove_sparse))
  return(remove_sparse_df)
}


cleaned_title <- clean_text(title_corpus)
if(ncol(cleaned_title) > 0){
  cleaned_title <- cleaned_title %>% rename_all(paste0, "_title")
}


cleaned_profile <- clean_text(profile_corpus)
if(ncol(cleaned_profile) > 0){
  cleaned_profile <- cleaned_profile %>% rename_all(paste0, "_profile")
}

cleaned_description <- clean_text(description_corpus)
if(ncol(cleaned_description) > 0){
  cleaned_description <- cleaned_description %>% rename_all(paste0, "_description")
}

cleaned_requirements <- clean_text(requirements_corpus)
if(ncol(cleaned_requirements) > 0){
  cleaned_requirements <- cleaned_requirements %>% rename_all(paste0, "_requirements")
}

cleaned_benefits <- clean_text(benefits_corpus)
if(ncol(cleaned_benefits) > 0){
  cleaned_benefits <- cleaned_benefits %>% rename_all(paste0, "_benefits")
}

```

```{r}
# Combine features
# the word features (from title, company_profile, description, requirements, benefits) 
# + binary features (telecommuting, has_company_logo, has_questions) 
# + categorical features (employment_type, required_experience, required_education)

library(plyr)
cleaned_description$fraudulent <- job$fraudulent 
features <- cbind(cleaned_title, cleaned_profile, cleaned_description, cleaned_requirements, cleaned_benefits, subset(job, select = c(telecommuting, has_company_logo, has_questions)), ctgr_dummy)
```



```{r}
# Feature selection using stepwise selection

features_model = lm(fraudulent ~., data = features)
library(MASS)
library(olsrr)
ols_model = ols_step_both_aic(features_model, details= TRUE)
ols_features = ols_model$predictors
plot(ols_model)
```


```{r}

library(tidyverse)

make_power_features <- function(df) {
  
  # country, state
  split_location <- strsplit(df$location, ", ")
  
  country <- c()
  for (i in 1:length(split_location)) {
    country <- c(country, split_location[[i]][1])
  }
  country[is.na(country)] <- 0
  
  state <- c()
  for (i in 1:length(split_location)) {
    state <- c(state, split_location[[i]][2])
  }
  state[is.na(state)] <- 0
  
  country_state <- data.frame(country = country, state = state)
  country_state[country_state$country != "US", "state"] <- ""
  
  # state_TX
  state_TX <- country_state$state
  state_TX <- ifelse(state_TX == "TX", 1, 0)
  #state_TX[state_TX != "TX"] <- 0
  #state_TX[state_TX == "TX"] <- 1
  state_TX = as.numeric(state_TX)
  
  # state_NY
  state_NY <- country_state$state
  state_NY <- ifelse(state_NY == "NY", 1, 0)
  #state_NY[state_NY != "NY"] <- 0
  #state_NY[state_NY == "NY"] <- 1
  state_NY = as.numeric(state_NY)
  
  # state_CA
  state_CA <- country_state$state
  state_CA <- ifelse(state_CA == "CA", 1, 0)
  #state_CA[state_CA != "CA"] <- 0
  #state_CA[state_CA == "CA"] <- 1
  state_CA = as.numeric(state_CA)
  
  # length_des
  length_des <- nchar(df$description)
  length_des[is.na(length_des)] <- 0
  
  # length_ben
  length_ben <- nchar(df$benefits)
  length_ben[is.na(length_ben)] <- 0
  
  # contain_email
  contain_email <- str_extract(df$company_profile, "#PHONE_(.*)#")
  contain_email[is.na(contain_email)] <- 0
  contain_email[contain_email != "0"] <- 1
  contain_email = as.numeric(contain_email)
  
  # length_req
  length_req <- nchar(df$requirements)
  length_req[is.na(length_req)] <- 0
  
  # contain_phone
  contain_phone <- str_extract(df$company_profile, "#EMAIL_(.*)#")
  contain_phone[is.na(contain_phone)] <- 0
  contain_phone[contain_phone != "0"] <- 1
  contain_phone = as.numeric(contain_phone)
  
  # has_salary
  has_salary <- ifelse(is.na(df$salary_range), 0, 1)
  
  oil_ind <- ifelse(df$industry == "Oil & Energy", 1, 0)
  oil_ind[is.na(oil_ind)] <- 0

  hos_ind <- ifelse(df$industry == "Hospital & Health Care", 1, 0)
  hos_ind[is.na(hos_ind)] <- 0

  acc_ind <- ifelse(df$industry == "Accounting", 1, 0)
  acc_ind[is.na(acc_ind)] <- 0

  oil_dept <- ifelse(df$department == "Oil & Energy", 1, 0)
  oil_dept[is.na(oil_dept)] <- 0
  
  length_profile <- nchar(df$company_profile)
  length_profile[is.na(length_profile)] <- 0

  eng_dept <- ifelse(df$department == "Engineering", 1, 0)
  eng_dept[is.na(eng_dept)] <- 0
  
  #customer_dept <- ifelse(df$department == "Customer Service", 1, 0)
  #customer_dept[is.na(customer_dept)] <- 0
  
  #clerical_dept <- ifelse(df$department =="Clerical", 1, 0)
  #clerical_dept[is.na(clerical_dept)] <- 0
  
  #acc_dept <- ifelse(df$department == "Account", 1, 0)
  #acc_dept[is.na(acc_dept)] <- 0
  
  #admin_dept <- ifelse(df$department == "admin", 1, 0)
  #admin_dept[is.na(admin_dept)] <- 0
  
  # uppercase_des
  upper_des <- lengths(str_extract_all(df$description, "[A-Z]{3,}+"))
  upper_des[is.na(upper_des)] <- 0


# uppercase_req
  upper_req <- lengths(str_extract_all(df$requirements, "[A-Z]{3,}+"))
  upper_req[is.na(upper_req)] <- 0


# uppercase_ben
  upper_ben <- lengths(str_extract_all(df$benefits, "[A-Z]{3,}+"))
  upper_ben[is.na(upper_ben)] <- 0
  
# star_des
  star_des <- as.numeric(grepl("*", df$description, fixed = TRUE))
  star_des[is.na(star_des)] <- 0


# star_req
  star_req <- as.numeric(grepl("*", df$requirements, fixed = TRUE))
  star_req[is.na(star_req)] <- 0

# star_ben
  star_ben <- as.numeric(grepl("*", df$benefits, fixed = TRUE))
  star_ben[is.na(star_ben)] <- 0

# na_company
  na_company <- as.numeric(is.na(df$company_profile))

  
  features <- data.frame(state_TX = state_TX,
                         length_des = length_des,
                         length_ben = length_ben,
                         state_NY = state_NY,
                         state_CA = state_CA,
                         contain_email = contain_email,
                         length_req = length_req,
                         contain_phone = contain_phone,
                         has_salary = has_salary,
                         oil_ind = oil_ind,
                         hos_ind = hos_ind,
                         acc_ind = acc_ind,
                         oil_dept = oil_dept,
                         length_profile = length_profile,
                         eng_dept = eng_dept,
                         upper_des = upper_des,
                         upper_req = upper_req,
                         upper_ben = upper_ben
                         )
  
  return(features)
  
}
```

```{r}
# power features
power_features <- make_power_features(job)
```

```{r}
final_features = cbind(subset(features, select = ols_model$predictors), power_features)
```



```{r}
# Split the dataset into train and test
final_features$fraudulent <- job$fraudulent

set.seed(12345)

split <- sample(c(TRUE, FALSE), nrow(final_features), replace=TRUE, prob=c(0.8, 0.2))
job_train <-  final_features[split,]
job_test <- final_features[!split, ]

job_train
job_test


job_train$fraudulent = as.factor(job_train$fraudulent)
job_test$fraudulent = as.factor(job_test$fraudulent)

final_features$fraudulent <- as.factor(job$fraudulent)
```

```{r}
write.csv(final_features, "final_features.csv")
```

```{r}
# SVM
library(e1071)
svm_model <- svm(fraudulent~., data=job_train , kernel ="radial", scale=TRUE)
summary(svm_model)

train_pred_svm <- predict(svm_model, subset(job_train, select = -fraudulent))
table(train_pred_svm, job_train$fraudulent)

test_pred_svm <- predict(svm_model, subset(job_test, select = -fraudulent))
table(test_pred_svm, job_test$fraudulent)
```


```{r}
# Random Forest Model
library(randomForest)

rf_model <- randomForest(fraudulent~., data=job_train, ntree=120, mtry = 25, importance =TRUE)

train_pred_rf <- predict(rf_model, subset(job_train, select = -fraudulent))
table(train_pred_rf, job_train$fraudulent)

test_pred_rf <- predict(rf_model ,subset(job_test, select = -fraudulent))
table(test_pred_rf, job_test$fraudulent)

```

```{r}
# Random Forest Model
library(randomForest)

rf_model <- randomForest(fraudulent~., data=final_features, cutoff = c(0.8, 0.2), mtry = 23, importance =TRUE)

saveRDS(rf_model, file = "rf_model1.RDS")
```



================================================
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tm)
library(SnowballC)
library(tidytext)
library(fastDummies)
library(plyr)
```

```{r}
combine_features<- function(job){
  categorical <- subset(job, select = c(employment_type, required_experience, required_education))
  ctgr_dummy <- dummy_cols(categorical, select_columns=c('employment_type', 'required_experience', 'required_education'),  remove_most_frequent_dummy = TRUE, remove_selected_columns = TRUE)
  ctgr_dummy[is.na(ctgr_dummy)] <- 0
  
  colnames(ctgr_dummy) <- gsub(" ", ".", colnames(ctgr_dummy))
  colnames(ctgr_dummy) <- gsub("-", ".", colnames(ctgr_dummy))
  colnames(ctgr_dummy) <- gsub("'", ".", colnames(ctgr_dummy))
  
  title_corpus <- VCorpus(VectorSource(job$title))
  profile_corpus <- VCorpus(VectorSource(job$company_profile))
  description_corpus <- VCorpus(VectorSource(job$description))
  requirements_corpus <- VCorpus(VectorSource(job$requirements))
  benefits_corpus <- VCorpus(VectorSource(job$benefits))
  
  clean_text <- function(corpus){
    corpus <- tm_map(corpus, content_transformer(tolower), lazy = T)
    corpus <- tm_map(corpus, removeNumbers, lazy = T)
    corpus <- tm_map(corpus, removePunctuation, lazy = T)
    corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"), lazy = T )
    corpus <- tm_map(corpus, stripWhitespace, lazy = T )
    corpus <- tm_map(corpus, stemDocument, lazy = T)
    corpus <- tm_map(corpus, stripWhitespace, lazy = T)
    word_freq <- DocumentTermMatrix(corpus)
    remove_sparse <- removeSparseTerms(word_freq, 0.8)
    remove_sparse_df <- as.data.frame(as.matrix(remove_sparse))
    return(remove_sparse_df)
  }
  
  
  cleaned_title <- clean_text(title_corpus)
  if(ncol(cleaned_title) > 0){
    cleaned_title <- cleaned_title %>% rename_all(paste0, "_title")
  }
  
  
  cleaned_profile <- clean_text(profile_corpus)
  if(ncol(cleaned_profile) > 0){
    cleaned_profile <- cleaned_profile %>% rename_all(paste0, "_profile")
  }
  
  cleaned_description <- clean_text(description_corpus)
  if(ncol(cleaned_description) > 0){
    cleaned_description <- cleaned_description %>% rename_all(paste0, "_description")
  }
  
  cleaned_requirements <- clean_text(requirements_corpus)
  if(ncol(cleaned_requirements) > 0){
    cleaned_requirements <- cleaned_requirements %>% rename_all(paste0, "_requirements")
  }
  
  cleaned_benefits <- clean_text(benefits_corpus)
  if(ncol(cleaned_benefits) > 0){
    cleaned_benefits <- cleaned_benefits %>% rename_all(paste0, "_benefits")
  }
  
  
  cleaned_description$fraudulent <- job$fraudulent 
  features <- cbind(cleaned_title, cleaned_profile, cleaned_description, cleaned_requirements, cleaned_benefits, subset(job, select = c(telecommuting, has_company_logo, has_questions)), ctgr_dummy)
  
  return(features)
}


```




```{r}
# Run CSV file
test2 = read_csv('job_verification_data.csv')

# Apply text mining on test data
combine_features(test2)
# Bind columns of test data set that are in our final features.
test_data = cbind(combine_features(test2)[colnames(combine_features(test2)) %in% colnames(final_features)],
                  make_power_features(test2)[colnames(make_power_features(test2)) %in% colnames(final_features)])

# Add columns that are not in final features with values 0

mat = matrix(data = 0, nrow = nrow(test2), ncol = length(colnames(final_features)[!colnames(final_features) %in% colnames(test_data)]))
mat = as.data.frame(mat)
colnames(mat) = colnames(final_features)[!colnames(final_features) %in% colnames(test_data)]
test_data = cbind(test_data, mat)


write.csv(test_data, "test_data.csv", row.names=FALSE)
```


```{r}
library(randomForest)
# Apply test set to our RF model
loaded_model = readRDS("rf_model1.RDS")

# New data after going through prescript data
test_data = read.csv("test_data.csv")
test_data

pred = predict(loaded_model, subset(test_data, select = -fraudulent))

# Accuracy within True class(sensitivity or True Positive Rate)
sum(pred == 1 & test_data$fraudulent == 1) / sum(test_data$fraudulent == 1)

# Accuracy with False Class
sum(pred == 0 & test_data$fraudulent == 0) / sum(test_data$fraudulent == 0)

# Overall Accuracy
sum(pred == test_data$fraudulent) / nrow(test_data)

```

============




```{r}
library(tidyverse)
library(cluster)
library(haven)
library(ggdendro)
library(NbClust)
library(factoextra)
library(klaR)
library(data.table)
library(rlang)
library(dplyr)
library(NbClust)
library(ggpubr)
library(corrplot)
ff = subset(final_features, select = -fraudulent) %>% scale()

ncol <- ncol(ff)
# Store variable names
var <- list()
for(i in 1:ncol){
   var[[i]] <- names(ff)[i]
}
# Rename columns for easier use
names(ff)[1:ncol] <- paste("var", 1:ncol, sep="")


######clustering
### Hierarchical Clustering 
## Determine the number of clusters(after scale)

# Elbow method
fviz_nbclust(ff, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(ff, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# Gap statistic
#set.seed(123)
#fviz_nbclust(ff, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
#  labs(subtitle = "Gap statistic method")
```


```{r}
#set cluster number as 4
m = 2
#cluster name
#c1 = "cluster1"
#c2 = "cluster2"
#c3 = "cluster3"
#c4 = "cluster4"

cluster <- ff %>% 
    dist(method = "euclidean") %>% 
    hclust(method="ward.D")

ggdendrogram(cluster)

#fviz_dend(cluster, k = m, # Cut in m groups
#          cex = 0.5, # label size
#          k_colors = c("#2E9FDF", "#00AFBB"),
#          color_labels_by_k = TRUE, # color labels by groups
#          rect = TRUE # Add rectangle around groups
#)
#, "#E7B800", "#FC4E07"
# Bind data with respective clusters
groups <- cutree(cluster, k=m) # cut tree into 5 clusters

databind <- cbind(ff, Cluster = groups)  # Bind data with respective clusters
head(databind)
dim(databind)
```




# K-fold


library(caret)
control <- trainControl(method="repeatedcv", number=10, search="grid")
#set.seed(seed)
tunegrid <- expand.grid(.mtry=c(22:25))
rf_gridsearch <- train(fraudulent~., data=final_features, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```











